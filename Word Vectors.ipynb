{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data from files\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting = 3)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting = 3)\n",
    "\n",
    "# Verify the data\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " train[\"review\"].size, unlabeled_train[\"review\"].size ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review,\"lxml\").get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/usr/local/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "print(\"Parsing sentences from training set\")\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "\n",
    "num_features = 300 # Word vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4 # Threads\n",
    "context = 10 # Context window size\n",
    "\n",
    "downsampling = 1e-3 # Downsampling setting fro frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-09 14:19:52,823 : INFO : collecting all words and their counts\n",
      "2018-03-09 14:19:52,826 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-09 14:19:52,938 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-09 14:19:53,050 : INFO : PROGRESS: at sentence #20000, processed 451867 words, keeping 24947 word types\n",
      "2018-03-09 14:19:53,166 : INFO : PROGRESS: at sentence #30000, processed 671290 words, keeping 30033 word types\n",
      "2018-03-09 14:19:53,284 : INFO : PROGRESS: at sentence #40000, processed 897790 words, keeping 34347 word types\n",
      "2018-03-09 14:19:53,393 : INFO : PROGRESS: at sentence #50000, processed 1116929 words, keeping 37760 word types\n",
      "2018-03-09 14:19:53,499 : INFO : PROGRESS: at sentence #60000, processed 1338370 words, keeping 40722 word types\n",
      "2018-03-09 14:19:53,618 : INFO : PROGRESS: at sentence #70000, processed 1561505 words, keeping 43332 word types\n",
      "2018-03-09 14:19:53,739 : INFO : PROGRESS: at sentence #80000, processed 1780812 words, keeping 45713 word types\n",
      "2018-03-09 14:19:53,852 : INFO : PROGRESS: at sentence #90000, processed 2004905 words, keeping 48134 word types\n",
      "2018-03-09 14:19:53,961 : INFO : PROGRESS: at sentence #100000, processed 2226863 words, keeping 50206 word types\n",
      "2018-03-09 14:19:54,070 : INFO : PROGRESS: at sentence #110000, processed 2446412 words, keeping 52077 word types\n",
      "2018-03-09 14:19:54,194 : INFO : PROGRESS: at sentence #120000, processed 2668549 words, keeping 54113 word types\n",
      "2018-03-09 14:19:54,324 : INFO : PROGRESS: at sentence #130000, processed 2894077 words, keeping 55841 word types\n",
      "2018-03-09 14:19:54,452 : INFO : PROGRESS: at sentence #140000, processed 3106779 words, keeping 57340 word types\n",
      "2018-03-09 14:19:54,583 : INFO : PROGRESS: at sentence #150000, processed 3332313 words, keeping 59049 word types\n",
      "2018-03-09 14:19:54,708 : INFO : PROGRESS: at sentence #160000, processed 3555001 words, keeping 60611 word types\n",
      "2018-03-09 14:19:54,829 : INFO : PROGRESS: at sentence #170000, processed 3778341 words, keeping 62071 word types\n",
      "2018-03-09 14:19:54,923 : INFO : PROGRESS: at sentence #180000, processed 3998922 words, keeping 63490 word types\n",
      "2018-03-09 14:19:55,010 : INFO : PROGRESS: at sentence #190000, processed 4224105 words, keeping 64788 word types\n",
      "2018-03-09 14:19:55,097 : INFO : PROGRESS: at sentence #200000, processed 4448225 words, keeping 66079 word types\n",
      "2018-03-09 14:19:55,185 : INFO : PROGRESS: at sentence #210000, processed 4669555 words, keeping 67383 word types\n",
      "2018-03-09 14:19:55,273 : INFO : PROGRESS: at sentence #220000, processed 4894556 words, keeping 68690 word types\n",
      "2018-03-09 14:19:55,363 : INFO : PROGRESS: at sentence #230000, processed 5117022 words, keeping 69950 word types\n",
      "2018-03-09 14:19:55,477 : INFO : PROGRESS: at sentence #240000, processed 5344527 words, keeping 71159 word types\n",
      "2018-03-09 14:19:55,604 : INFO : PROGRESS: at sentence #250000, processed 5558635 words, keeping 72343 word types\n",
      "2018-03-09 14:19:55,755 : INFO : PROGRESS: at sentence #260000, processed 5778616 words, keeping 73470 word types\n",
      "2018-03-09 14:19:55,895 : INFO : PROGRESS: at sentence #270000, processed 5999905 words, keeping 74759 word types\n",
      "2018-03-09 14:19:56,034 : INFO : PROGRESS: at sentence #280000, processed 6225784 words, keeping 76361 word types\n",
      "2018-03-09 14:19:56,183 : INFO : PROGRESS: at sentence #290000, processed 6448944 words, keeping 77831 word types\n",
      "2018-03-09 14:19:56,333 : INFO : PROGRESS: at sentence #300000, processed 6673547 words, keeping 79163 word types\n",
      "2018-03-09 14:19:56,480 : INFO : PROGRESS: at sentence #310000, processed 6898861 words, keeping 80472 word types\n",
      "2018-03-09 14:19:56,621 : INFO : PROGRESS: at sentence #320000, processed 7123745 words, keeping 81800 word types\n",
      "2018-03-09 14:19:56,773 : INFO : PROGRESS: at sentence #330000, processed 7345404 words, keeping 83022 word types\n",
      "2018-03-09 14:19:56,893 : INFO : PROGRESS: at sentence #340000, processed 7574852 words, keeping 84272 word types\n",
      "2018-03-09 14:19:57,012 : INFO : PROGRESS: at sentence #350000, processed 7798104 words, keeping 85417 word types\n",
      "2018-03-09 14:19:57,122 : INFO : PROGRESS: at sentence #360000, processed 8018679 words, keeping 86587 word types\n",
      "2018-03-09 14:19:57,232 : INFO : PROGRESS: at sentence #370000, processed 8245856 words, keeping 87699 word types\n",
      "2018-03-09 14:19:57,332 : INFO : PROGRESS: at sentence #380000, processed 8470979 words, keeping 88869 word types\n",
      "2018-03-09 14:19:57,436 : INFO : PROGRESS: at sentence #390000, processed 8700551 words, keeping 89893 word types\n",
      "2018-03-09 14:19:57,537 : INFO : PROGRESS: at sentence #400000, processed 8923483 words, keeping 90902 word types\n",
      "2018-03-09 14:19:57,659 : INFO : PROGRESS: at sentence #410000, processed 9144826 words, keeping 91866 word types\n",
      "2018-03-09 14:19:57,781 : INFO : PROGRESS: at sentence #420000, processed 9365858 words, keeping 92899 word types\n",
      "2018-03-09 14:19:57,896 : INFO : PROGRESS: at sentence #430000, processed 9593338 words, keeping 93919 word types\n",
      "2018-03-09 14:19:58,020 : INFO : PROGRESS: at sentence #440000, processed 9820034 words, keeping 94893 word types\n",
      "2018-03-09 14:19:58,150 : INFO : PROGRESS: at sentence #450000, processed 10043796 words, keeping 96024 word types\n",
      "2018-03-09 14:19:58,289 : INFO : PROGRESS: at sentence #460000, processed 10276556 words, keeping 97076 word types\n",
      "2018-03-09 14:19:58,401 : INFO : PROGRESS: at sentence #470000, processed 10504440 words, keeping 97921 word types\n",
      "2018-03-09 14:19:58,512 : INFO : PROGRESS: at sentence #480000, processed 10724824 words, keeping 98850 word types\n",
      "2018-03-09 14:19:58,630 : INFO : PROGRESS: at sentence #490000, processed 10951484 words, keeping 99858 word types\n",
      "2018-03-09 14:19:58,737 : INFO : PROGRESS: at sentence #500000, processed 11173140 words, keeping 100752 word types\n",
      "2018-03-09 14:19:58,827 : INFO : PROGRESS: at sentence #510000, processed 11398393 words, keeping 101685 word types\n",
      "2018-03-09 14:19:58,908 : INFO : PROGRESS: at sentence #520000, processed 11621731 words, keeping 102584 word types\n",
      "2018-03-09 14:19:58,984 : INFO : PROGRESS: at sentence #530000, processed 11846081 words, keeping 103386 word types\n",
      "2018-03-09 14:19:59,062 : INFO : PROGRESS: at sentence #540000, processed 12070696 words, keeping 104251 word types\n",
      "2018-03-09 14:19:59,145 : INFO : PROGRESS: at sentence #550000, processed 12296195 words, keeping 105117 word types\n",
      "2018-03-09 14:19:59,222 : INFO : PROGRESS: at sentence #560000, processed 12517476 words, keeping 105981 word types\n",
      "2018-03-09 14:19:59,308 : INFO : PROGRESS: at sentence #570000, processed 12746461 words, keeping 106771 word types\n",
      "2018-03-09 14:19:59,402 : INFO : PROGRESS: at sentence #580000, processed 12967949 words, keeping 107650 word types\n",
      "2018-03-09 14:19:59,496 : INFO : PROGRESS: at sentence #590000, processed 13193474 words, keeping 108486 word types\n",
      "2018-03-09 14:19:59,582 : INFO : PROGRESS: at sentence #600000, processed 13415660 words, keeping 109203 word types\n",
      "2018-03-09 14:19:59,658 : INFO : PROGRESS: at sentence #610000, processed 13636656 words, keeping 110077 word types\n",
      "2018-03-09 14:19:59,743 : INFO : PROGRESS: at sentence #620000, processed 13862938 words, keeping 110822 word types\n",
      "2018-03-09 14:19:59,818 : INFO : PROGRESS: at sentence #630000, processed 14087222 words, keeping 111596 word types\n",
      "2018-03-09 14:19:59,895 : INFO : PROGRESS: at sentence #640000, processed 14307942 words, keeping 112402 word types\n",
      "2018-03-09 14:19:59,979 : INFO : PROGRESS: at sentence #650000, processed 14533624 words, keeping 113182 word types\n",
      "2018-03-09 14:20:00,060 : INFO : PROGRESS: at sentence #660000, processed 14756308 words, keeping 113932 word types\n",
      "2018-03-09 14:20:00,143 : INFO : PROGRESS: at sentence #670000, processed 14979692 words, keeping 114630 word types\n",
      "2018-03-09 14:20:00,225 : INFO : PROGRESS: at sentence #680000, processed 15204499 words, keeping 115341 word types\n",
      "2018-03-09 14:20:00,308 : INFO : PROGRESS: at sentence #690000, processed 15426666 words, keeping 116118 word types\n",
      "2018-03-09 14:20:00,386 : INFO : PROGRESS: at sentence #700000, processed 15655301 words, keeping 116930 word types\n",
      "2018-03-09 14:20:00,468 : INFO : PROGRESS: at sentence #710000, processed 15878274 words, keeping 117583 word types\n",
      "2018-03-09 14:20:00,551 : INFO : PROGRESS: at sentence #720000, processed 16103516 words, keeping 118207 word types\n",
      "2018-03-09 14:20:00,626 : INFO : PROGRESS: at sentence #730000, processed 16329897 words, keeping 118940 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-09 14:20:00,700 : INFO : PROGRESS: at sentence #740000, processed 16550913 words, keeping 119654 word types\n",
      "2018-03-09 14:20:00,777 : INFO : PROGRESS: at sentence #750000, processed 16769240 words, keeping 120282 word types\n",
      "2018-03-09 14:20:00,858 : INFO : PROGRESS: at sentence #760000, processed 16988632 words, keeping 120917 word types\n",
      "2018-03-09 14:20:00,935 : INFO : PROGRESS: at sentence #770000, processed 17215761 words, keeping 121690 word types\n",
      "2018-03-09 14:20:01,025 : INFO : PROGRESS: at sentence #780000, processed 17445902 words, keeping 122389 word types\n",
      "2018-03-09 14:20:01,123 : INFO : PROGRESS: at sentence #790000, processed 17672895 words, keeping 123055 word types\n",
      "2018-03-09 14:20:01,180 : INFO : collected 123493 word types from a corpus of 17795898 raw words and 795538 sentences\n",
      "2018-03-09 14:20:01,181 : INFO : Loading a fresh vocabulary\n",
      "2018-03-09 14:20:02,778 : INFO : min_count=40 retains 16490 unique words (13% of original 123493, drops 107003)\n",
      "2018-03-09 14:20:02,780 : INFO : min_count=40 leaves 17236863 word corpus (96% of original 17795898, drops 559035)\n",
      "2018-03-09 14:20:02,906 : INFO : deleting the raw counts dictionary of 123493 items\n",
      "2018-03-09 14:20:02,914 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-03-09 14:20:02,916 : INFO : downsampling leaves estimated 12748070 word corpus (74.0% of prior 17236863)\n",
      "2018-03-09 14:20:03,000 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2018-03-09 14:20:03,002 : INFO : resetting layer weights\n",
      "2018-03-09 14:20:03,584 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-03-09 14:20:04,600 : INFO : EPOCH 1 - PROGRESS: at 2.85% examples, 362420 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:20:05,607 : INFO : EPOCH 1 - PROGRESS: at 5.76% examples, 365736 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:06,607 : INFO : EPOCH 1 - PROGRESS: at 8.67% examples, 365174 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:07,620 : INFO : EPOCH 1 - PROGRESS: at 11.26% examples, 355063 words/s, in_qsize 7, out_qsize 2\n",
      "2018-03-09 14:20:08,638 : INFO : EPOCH 1 - PROGRESS: at 14.11% examples, 354400 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:20:09,649 : INFO : EPOCH 1 - PROGRESS: at 17.35% examples, 362477 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:10,702 : INFO : EPOCH 1 - PROGRESS: at 20.27% examples, 361055 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:11,730 : INFO : EPOCH 1 - PROGRESS: at 23.53% examples, 366441 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:12,746 : INFO : EPOCH 1 - PROGRESS: at 26.35% examples, 364793 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:13,760 : INFO : EPOCH 1 - PROGRESS: at 29.53% examples, 368513 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:14,763 : INFO : EPOCH 1 - PROGRESS: at 32.57% examples, 369294 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-09 14:20:15,768 : INFO : EPOCH 1 - PROGRESS: at 35.56% examples, 369928 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:20:16,781 : INFO : EPOCH 1 - PROGRESS: at 38.34% examples, 368595 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:17,793 : INFO : EPOCH 1 - PROGRESS: at 40.75% examples, 363979 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:20:18,794 : INFO : EPOCH 1 - PROGRESS: at 43.20% examples, 360694 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:19,805 : INFO : EPOCH 1 - PROGRESS: at 46.00% examples, 360288 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:20:20,811 : INFO : EPOCH 1 - PROGRESS: at 48.61% examples, 358743 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:21,835 : INFO : EPOCH 1 - PROGRESS: at 51.14% examples, 356271 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:22,901 : INFO : EPOCH 1 - PROGRESS: at 52.78% examples, 347329 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-09 14:20:23,923 : INFO : EPOCH 1 - PROGRESS: at 54.05% examples, 337950 words/s, in_qsize 6, out_qsize 3\n",
      "2018-03-09 14:20:24,925 : INFO : EPOCH 1 - PROGRESS: at 56.06% examples, 334155 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:25,941 : INFO : EPOCH 1 - PROGRESS: at 58.07% examples, 330784 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:27,004 : INFO : EPOCH 1 - PROGRESS: at 59.79% examples, 325239 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:20:28,021 : INFO : EPOCH 1 - PROGRESS: at 61.81% examples, 322239 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:29,023 : INFO : EPOCH 1 - PROGRESS: at 64.06% examples, 320778 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:30,044 : INFO : EPOCH 1 - PROGRESS: at 66.40% examples, 319778 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:31,050 : INFO : EPOCH 1 - PROGRESS: at 68.73% examples, 318736 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:20:32,063 : INFO : EPOCH 1 - PROGRESS: at 71.00% examples, 317705 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:33,092 : INFO : EPOCH 1 - PROGRESS: at 73.34% examples, 316798 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:34,096 : INFO : EPOCH 1 - PROGRESS: at 75.59% examples, 315746 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:35,106 : INFO : EPOCH 1 - PROGRESS: at 77.78% examples, 314476 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:36,121 : INFO : EPOCH 1 - PROGRESS: at 79.98% examples, 313228 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:37,169 : INFO : EPOCH 1 - PROGRESS: at 82.28% examples, 312189 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:38,181 : INFO : EPOCH 1 - PROGRESS: at 84.52% examples, 311322 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:39,186 : INFO : EPOCH 1 - PROGRESS: at 86.77% examples, 310572 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:40,194 : INFO : EPOCH 1 - PROGRESS: at 88.99% examples, 309828 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:41,200 : INFO : EPOCH 1 - PROGRESS: at 91.27% examples, 309336 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:20:42,211 : INFO : EPOCH 1 - PROGRESS: at 93.51% examples, 308645 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:43,241 : INFO : EPOCH 1 - PROGRESS: at 95.48% examples, 306759 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:44,265 : INFO : EPOCH 1 - PROGRESS: at 97.57% examples, 305708 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:45,282 : INFO : EPOCH 1 - PROGRESS: at 99.44% examples, 304100 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-09 14:20:45,489 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-09 14:20:45,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-09 14:20:45,534 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-09 14:20:45,560 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-09 14:20:45,562 : INFO : EPOCH - 1 : training on 17795898 raw words (12747372 effective words) took 42.0s, 303745 effective words/s\n",
      "2018-03-09 14:20:46,603 : INFO : EPOCH 2 - PROGRESS: at 2.18% examples, 272577 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:47,612 : INFO : EPOCH 2 - PROGRESS: at 4.43% examples, 278174 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:48,616 : INFO : EPOCH 2 - PROGRESS: at 6.74% examples, 282655 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:49,639 : INFO : EPOCH 2 - PROGRESS: at 9.12% examples, 285440 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:50,660 : INFO : EPOCH 2 - PROGRESS: at 11.44% examples, 285797 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:51,669 : INFO : EPOCH 2 - PROGRESS: at 13.71% examples, 285549 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:52,693 : INFO : EPOCH 2 - PROGRESS: at 16.01% examples, 285665 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-09 14:20:53,706 : INFO : EPOCH 2 - PROGRESS: at 18.42% examples, 286956 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:20:54,719 : INFO : EPOCH 2 - PROGRESS: at 20.73% examples, 287217 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:20:55,748 : INFO : EPOCH 2 - PROGRESS: at 22.53% examples, 280663 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:20:56,802 : INFO : EPOCH 2 - PROGRESS: at 24.21% examples, 273412 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:20:57,827 : INFO : EPOCH 2 - PROGRESS: at 26.13% examples, 270381 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-09 14:20:58,841 : INFO : EPOCH 2 - PROGRESS: at 27.86% examples, 266409 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-09 14:20:59,873 : INFO : EPOCH 2 - PROGRESS: at 29.53% examples, 262165 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:00,908 : INFO : EPOCH 2 - PROGRESS: at 31.51% examples, 260321 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-09 14:21:01,916 : INFO : EPOCH 2 - PROGRESS: at 33.32% examples, 258251 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:02,917 : INFO : EPOCH 2 - PROGRESS: at 35.11% examples, 256527 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:03,970 : INFO : EPOCH 2 - PROGRESS: at 36.85% examples, 253885 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-09 14:21:05,009 : INFO : EPOCH 2 - PROGRESS: at 38.40% examples, 250598 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:06,029 : INFO : EPOCH 2 - PROGRESS: at 39.96% examples, 247905 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:07,039 : INFO : EPOCH 2 - PROGRESS: at 41.76% examples, 246882 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:08,042 : INFO : EPOCH 2 - PROGRESS: at 43.52% examples, 246050 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:09,047 : INFO : EPOCH 2 - PROGRESS: at 45.35% examples, 245261 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:10,071 : INFO : EPOCH 2 - PROGRESS: at 46.99% examples, 243748 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:11,094 : INFO : EPOCH 2 - PROGRESS: at 48.82% examples, 243230 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:12,103 : INFO : EPOCH 2 - PROGRESS: at 50.80% examples, 243426 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:13,106 : INFO : EPOCH 2 - PROGRESS: at 52.67% examples, 243107 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:14,116 : INFO : EPOCH 2 - PROGRESS: at 54.49% examples, 242759 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:15,155 : INFO : EPOCH 2 - PROGRESS: at 56.44% examples, 242689 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:16,206 : INFO : EPOCH 2 - PROGRESS: at 58.35% examples, 242524 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:17,242 : INFO : EPOCH 2 - PROGRESS: at 60.64% examples, 243852 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:18,262 : INFO : EPOCH 2 - PROGRESS: at 63.00% examples, 245420 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:19,272 : INFO : EPOCH 2 - PROGRESS: at 65.35% examples, 246980 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:20,299 : INFO : EPOCH 2 - PROGRESS: at 67.70% examples, 248329 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:21,301 : INFO : EPOCH 2 - PROGRESS: at 70.00% examples, 249573 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:22,330 : INFO : EPOCH 2 - PROGRESS: at 72.15% examples, 250181 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-09 14:21:23,341 : INFO : EPOCH 2 - PROGRESS: at 74.29% examples, 250677 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:24,361 : INFO : EPOCH 2 - PROGRESS: at 76.44% examples, 251102 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:25,378 : INFO : EPOCH 2 - PROGRESS: at 78.68% examples, 251874 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:26,402 : INFO : EPOCH 2 - PROGRESS: at 80.82% examples, 252218 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-09 14:21:27,424 : INFO : EPOCH 2 - PROGRESS: at 82.78% examples, 252045 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:28,433 : INFO : EPOCH 2 - PROGRESS: at 84.84% examples, 252280 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:29,438 : INFO : EPOCH 2 - PROGRESS: at 87.15% examples, 253186 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:30,446 : INFO : EPOCH 2 - PROGRESS: at 89.92% examples, 255473 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:31,457 : INFO : EPOCH 2 - PROGRESS: at 92.90% examples, 258108 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:32,475 : INFO : EPOCH 2 - PROGRESS: at 95.60% examples, 259669 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:33,479 : INFO : EPOCH 2 - PROGRESS: at 98.18% examples, 261242 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:34,145 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-09 14:21:34,175 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-09 14:21:34,184 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-09 14:21:34,186 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-09 14:21:34,187 : INFO : EPOCH - 2 : training on 17795898 raw words (12747472 effective words) took 48.6s, 262264 effective words/s\n",
      "2018-03-09 14:21:35,203 : INFO : EPOCH 3 - PROGRESS: at 2.62% examples, 335671 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:36,218 : INFO : EPOCH 3 - PROGRESS: at 5.48% examples, 347565 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:37,231 : INFO : EPOCH 3 - PROGRESS: at 8.39% examples, 351662 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:38,242 : INFO : EPOCH 3 - PROGRESS: at 11.20% examples, 352198 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:39,267 : INFO : EPOCH 3 - PROGRESS: at 14.17% examples, 354492 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:40,281 : INFO : EPOCH 3 - PROGRESS: at 17.18% examples, 357710 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:41,294 : INFO : EPOCH 3 - PROGRESS: at 20.05% examples, 357893 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:42,295 : INFO : EPOCH 3 - PROGRESS: at 22.76% examples, 356072 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:43,309 : INFO : EPOCH 3 - PROGRESS: at 25.63% examples, 356441 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:44,310 : INFO : EPOCH 3 - PROGRESS: at 28.43% examples, 356508 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:45,315 : INFO : EPOCH 3 - PROGRESS: at 31.38% examples, 357733 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:46,343 : INFO : EPOCH 3 - PROGRESS: at 34.28% examples, 357455 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:47,365 : INFO : EPOCH 3 - PROGRESS: at 37.22% examples, 358485 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:48,365 : INFO : EPOCH 3 - PROGRESS: at 40.02% examples, 358437 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:49,367 : INFO : EPOCH 3 - PROGRESS: at 42.58% examples, 356431 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:50,382 : INFO : EPOCH 3 - PROGRESS: at 45.46% examples, 356615 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:51,384 : INFO : EPOCH 3 - PROGRESS: at 48.32% examples, 357428 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:52,423 : INFO : EPOCH 3 - PROGRESS: at 50.80% examples, 354355 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:53,434 : INFO : EPOCH 3 - PROGRESS: at 53.44% examples, 353181 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:54,440 : INFO : EPOCH 3 - PROGRESS: at 55.67% examples, 349765 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:55,483 : INFO : EPOCH 3 - PROGRESS: at 58.14% examples, 347716 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:56,487 : INFO : EPOCH 3 - PROGRESS: at 60.80% examples, 347434 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:57,489 : INFO : EPOCH 3 - PROGRESS: at 62.94% examples, 344164 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:21:58,503 : INFO : EPOCH 3 - PROGRESS: at 65.18% examples, 341586 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:21:59,520 : INFO : EPOCH 3 - PROGRESS: at 67.81% examples, 341128 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:00,531 : INFO : EPOCH 3 - PROGRESS: at 70.22% examples, 339715 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:01,550 : INFO : EPOCH 3 - PROGRESS: at 73.00% examples, 340152 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:02,555 : INFO : EPOCH 3 - PROGRESS: at 75.59% examples, 339693 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:03,596 : INFO : EPOCH 3 - PROGRESS: at 78.06% examples, 338361 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-09 14:22:04,619 : INFO : EPOCH 3 - PROGRESS: at 80.59% examples, 337567 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-09 14:22:05,631 : INFO : EPOCH 3 - PROGRESS: at 82.95% examples, 336259 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-09 14:22:06,655 : INFO : EPOCH 3 - PROGRESS: at 85.52% examples, 335798 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:07,677 : INFO : EPOCH 3 - PROGRESS: at 88.03% examples, 335145 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:08,684 : INFO : EPOCH 3 - PROGRESS: at 90.71% examples, 335293 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:09,719 : INFO : EPOCH 3 - PROGRESS: at 93.64% examples, 335992 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-09 14:22:10,720 : INFO : EPOCH 3 - PROGRESS: at 96.04% examples, 335007 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:11,728 : INFO : EPOCH 3 - PROGRESS: at 98.77% examples, 335530 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:12,082 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-09 14:22:12,108 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-09 14:22:12,121 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-09 14:22:12,135 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-09 14:22:12,137 : INFO : EPOCH - 3 : training on 17795898 raw words (12747548 effective words) took 37.9s, 336036 effective words/s\n",
      "2018-03-09 14:22:13,181 : INFO : EPOCH 4 - PROGRESS: at 2.45% examples, 307696 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-09 14:22:14,211 : INFO : EPOCH 4 - PROGRESS: at 4.59% examples, 285770 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:15,216 : INFO : EPOCH 4 - PROGRESS: at 6.80% examples, 282917 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:16,259 : INFO : EPOCH 4 - PROGRESS: at 9.34% examples, 289434 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:17,274 : INFO : EPOCH 4 - PROGRESS: at 11.95% examples, 296360 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:18,294 : INFO : EPOCH 4 - PROGRESS: at 14.51% examples, 299607 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:19,322 : INFO : EPOCH 4 - PROGRESS: at 16.67% examples, 294517 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:20,337 : INFO : EPOCH 4 - PROGRESS: at 19.04% examples, 294625 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:21,353 : INFO : EPOCH 4 - PROGRESS: at 21.86% examples, 300989 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:22,366 : INFO : EPOCH 4 - PROGRESS: at 24.38% examples, 302662 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:23,372 : INFO : EPOCH 4 - PROGRESS: at 26.80% examples, 302904 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:24,383 : INFO : EPOCH 4 - PROGRESS: at 29.65% examples, 307654 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:25,395 : INFO : EPOCH 4 - PROGRESS: at 32.13% examples, 307366 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:26,405 : INFO : EPOCH 4 - PROGRESS: at 34.82% examples, 309631 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:27,439 : INFO : EPOCH 4 - PROGRESS: at 37.06% examples, 307374 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-09 14:22:28,443 : INFO : EPOCH 4 - PROGRESS: at 39.86% examples, 310390 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:29,449 : INFO : EPOCH 4 - PROGRESS: at 42.64% examples, 313005 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:30,488 : INFO : EPOCH 4 - PROGRESS: at 45.55% examples, 315547 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:31,517 : INFO : EPOCH 4 - PROGRESS: at 48.50% examples, 318354 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:32,529 : INFO : EPOCH 4 - PROGRESS: at 51.42% examples, 320808 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:33,537 : INFO : EPOCH 4 - PROGRESS: at 54.31% examples, 323056 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:34,559 : INFO : EPOCH 4 - PROGRESS: at 57.26% examples, 325237 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:35,571 : INFO : EPOCH 4 - PROGRESS: at 60.18% examples, 327392 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:36,579 : INFO : EPOCH 4 - PROGRESS: at 62.89% examples, 327925 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-09 14:22:37,599 : INFO : EPOCH 4 - PROGRESS: at 65.79% examples, 329404 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:38,616 : INFO : EPOCH 4 - PROGRESS: at 68.78% examples, 331068 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:39,616 : INFO : EPOCH 4 - PROGRESS: at 71.60% examples, 332303 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:40,629 : INFO : EPOCH 4 - PROGRESS: at 74.58% examples, 333792 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:41,659 : INFO : EPOCH 4 - PROGRESS: at 77.55% examples, 334997 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:42,672 : INFO : EPOCH 4 - PROGRESS: at 80.37% examples, 335607 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:43,697 : INFO : EPOCH 4 - PROGRESS: at 83.35% examples, 336707 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:44,700 : INFO : EPOCH 4 - PROGRESS: at 86.26% examples, 337765 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:45,719 : INFO : EPOCH 4 - PROGRESS: at 89.16% examples, 338572 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:46,720 : INFO : EPOCH 4 - PROGRESS: at 91.99% examples, 339324 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:47,725 : INFO : EPOCH 4 - PROGRESS: at 94.95% examples, 340197 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:48,744 : INFO : EPOCH 4 - PROGRESS: at 97.89% examples, 341068 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:49,417 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-09 14:22:49,442 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-09 14:22:49,450 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-09 14:22:49,456 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-09 14:22:49,458 : INFO : EPOCH - 4 : training on 17795898 raw words (12749225 effective words) took 37.3s, 341799 effective words/s\n",
      "2018-03-09 14:22:50,503 : INFO : EPOCH 5 - PROGRESS: at 2.85% examples, 354298 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:51,510 : INFO : EPOCH 5 - PROGRESS: at 5.81% examples, 365122 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:52,540 : INFO : EPOCH 5 - PROGRESS: at 8.83% examples, 365946 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:53,543 : INFO : EPOCH 5 - PROGRESS: at 11.84% examples, 368978 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:54,553 : INFO : EPOCH 5 - PROGRESS: at 14.78% examples, 368872 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:55,566 : INFO : EPOCH 5 - PROGRESS: at 17.80% examples, 369770 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:56,568 : INFO : EPOCH 5 - PROGRESS: at 20.73% examples, 369941 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:22:57,590 : INFO : EPOCH 5 - PROGRESS: at 23.70% examples, 370128 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:58,620 : INFO : EPOCH 5 - PROGRESS: at 26.75% examples, 370613 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:22:59,630 : INFO : EPOCH 5 - PROGRESS: at 29.70% examples, 371035 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:00,633 : INFO : EPOCH 5 - PROGRESS: at 32.69% examples, 370981 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:01,646 : INFO : EPOCH 5 - PROGRESS: at 35.68% examples, 371194 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:02,665 : INFO : EPOCH 5 - PROGRESS: at 38.63% examples, 371285 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:03,666 : INFO : EPOCH 5 - PROGRESS: at 41.54% examples, 371268 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:23:04,686 : INFO : EPOCH 5 - PROGRESS: at 44.50% examples, 371324 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:23:05,700 : INFO : EPOCH 5 - PROGRESS: at 47.45% examples, 371428 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:23:06,709 : INFO : EPOCH 5 - PROGRESS: at 50.34% examples, 371246 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:23:07,710 : INFO : EPOCH 5 - PROGRESS: at 53.33% examples, 371611 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:08,718 : INFO : EPOCH 5 - PROGRESS: at 56.21% examples, 371454 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:09,719 : INFO : EPOCH 5 - PROGRESS: at 59.06% examples, 371462 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:10,731 : INFO : EPOCH 5 - PROGRESS: at 61.97% examples, 371287 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:23:11,731 : INFO : EPOCH 5 - PROGRESS: at 64.90% examples, 371311 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-09 14:23:12,740 : INFO : EPOCH 5 - PROGRESS: at 67.75% examples, 370885 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:13,742 : INFO : EPOCH 5 - PROGRESS: at 70.66% examples, 370897 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:14,757 : INFO : EPOCH 5 - PROGRESS: at 73.63% examples, 371006 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:15,780 : INFO : EPOCH 5 - PROGRESS: at 76.61% examples, 370991 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-09 14:23:16,792 : INFO : EPOCH 5 - PROGRESS: at 79.58% examples, 371133 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:17,802 : INFO : EPOCH 5 - PROGRESS: at 81.82% examples, 367992 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:18,816 : INFO : EPOCH 5 - PROGRESS: at 84.18% examples, 365520 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:19,820 : INFO : EPOCH 5 - PROGRESS: at 86.93% examples, 364970 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:20,837 : INFO : EPOCH 5 - PROGRESS: at 89.54% examples, 363856 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:21,848 : INFO : EPOCH 5 - PROGRESS: at 91.99% examples, 362219 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:22,883 : INFO : EPOCH 5 - PROGRESS: at 94.55% examples, 360628 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:23,900 : INFO : EPOCH 5 - PROGRESS: at 96.97% examples, 358893 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:24,908 : INFO : EPOCH 5 - PROGRESS: at 99.61% examples, 358368 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-09 14:23:25,005 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-09 14:23:25,025 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-09 14:23:25,039 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-09 14:23:25,049 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-09 14:23:25,051 : INFO : EPOCH - 5 : training on 17795898 raw words (12747454 effective words) took 35.6s, 358313 effective words/s\n",
      "2018-03-09 14:23:25,052 : INFO : training on a 88979490 raw words (63739071 effective words) took 201.5s, 316384 effective words/s\n",
      "2018-03-09 14:23:25,070 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-03-09 14:23:25,509 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-03-09 14:23:25,511 : INFO : not storing attribute vectors_norm\n",
      "2018-03-09 14:23:25,514 : INFO : not storing attribute cum_table\n",
      "2018-03-09 14:23:26,563 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training Model ... \")\n",
    "\n",
    "model = word2vec.Word2Vec(sentences,workers=num_workers, \\\n",
    "                         size=num_features, min_count = min_word_count, \\\n",
    "                         window = context, sample=downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6042462587356567),\n",
       " ('lady', 0.5923141837120056),\n",
       " ('lad', 0.5652726292610168),\n",
       " ('monk', 0.5444037914276123),\n",
       " ('farmer', 0.5356318354606628),\n",
       " ('chap', 0.5209022760391235),\n",
       " ('businessman', 0.5208353996276855),\n",
       " ('guy', 0.514893651008606),\n",
       " ('boxer', 0.509599506855011),\n",
       " ('soldier', 0.5093779563903809)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6770464181900024),\n",
       " ('belle', 0.6076541543006897),\n",
       " ('bride', 0.6059161424636841),\n",
       " ('victoria', 0.5874012112617493),\n",
       " ('maria', 0.5845478773117065),\n",
       " ('maid', 0.5812250375747681),\n",
       " ('starlet', 0.5796318650245667),\n",
       " ('mistress', 0.5777221322059631),\n",
       " ('duchess', 0.5757953524589539),\n",
       " ('eva', 0.5731122493743896)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.wv.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
